#+STARTUP: overview
#+STARTUP: hidestars
#+OPTIONS: LaTeX:t
#+OPTIONS: toc:nil
#+LaTeX_CLASS: per-file-class

#+TITLE: Заявка на получение стипендии
#+AUTHOR: Кузнецов М.А.
#+DATE: 
* LATEX OPTIONS 						   :noexport:
#+OPTIONS: toc:nil
** Packages
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{placeins}
#+LATEX_HEADER: \usepackage[T2A]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[english,russian]{babel}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amsfonts,amsmath,amssymb}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{algorithmic} \usepackage[ruled]{algorithm}
#+LATEX_HEADER: \usepackage[unicode=true,plainpages=false]{hyperref}
#+LATEX_HEADER: \hypersetup{colorlinks=true,linkcolor=magenta,anchorcolor=magenta,urlcolor=blue,citecolor=blue}
** User-defined symbols
#+LATEX_HEADER: \def\A{\mathbf{A}}
#+LATEX_HEADER: \def\V{\mathbf{V}}
#+LATEX_HEADER: \def\B{\mathbf{B}}
#+LATEX_HEADER: \def\C{\mathbf{C}}
** Geometry
#+LATEX_HEADER: \usepackage[left=2.5cm,top=2cm,right=2cm,bottom=2cm,a4paper]{geometry}
#+LATEX_HEADER: \input{mytitle}







* Introduction
To solve modern computer tasks it is neсessary to use huge
computational power. One of the most effective computational tools are
graphics processors, however, despite the development of design tools,
creating GPU-code takes a lot of time. In itself, programming on GPU
is a difficult task, so would like to have the technology of
automatic parallelization, but they often lose the "manual"
programming. It is possible to single out a class of problems, which
is subject to automatic generation of effective GPU-code, and the
resulting code could be used in dynamical languages (such as Python
--- [[http://www.python.org]]). Until recently such a convenient tool was
not, but they've started to develop rapidly. We take as a basis
project loopy ([[http://git.tiker.net/loopy.git]]), designed by Andreas
Kloekner ([[http://mathema.tician.de/aboutme/]]), that could generate
OpenCL Python-modules for small, but
laborious cycles.


* Purpose of the work
\bfseries{The purpose of the work is analysis and development of the
technologies for the autogeneration of the GPU code with application
to the problems of the tensor approximation.}\mdseries

Tensor algorithms are laborious and require a large number of calculation,
that is why the development of parallel versions of standard
algorithms plays an important role, nevertheless a very few parallel
program versions implementing such algorithms exists. As a programming
language Python is used, as auto-generating code tool loopy is.

Python choice caused by the fact that Python has a number of
advantages over the standard languages (C, Fortran) such as: ease of
development and writing the new code, the presence of the standard
library, and more. Python is enough slow (because of the nature of
organization of loops, variable definitions) and the idea of using it for
parallel programming may seem unfortunate. However, the power of
Python is the ability to include the modules written in C, Fortran, as
well as packages of automatic generation of such modules thereby it could
keep the simplicity of Python and get the execution speed of
C-code. In particular pyOpenCL
([[http://mathema.tician.de/software/pyopencl]]) and pyCUDA
([[http://mathema.tician.de/software/pycuda]]) modules are implemented in
Python. 

It should be noted that automatically generated code is
usually inferior to "manual", however the automatic code generation
provides high efficiency with parallelizing loops. It also reduce
development time. It also should be said that \bfseries GPU-code
generated by Loopy is large and complex and it would take a long time
to write an analog "by hand" \mdseries, while \bfseries the duration
of  Loopy work is measured in seconds \mdseries. However, \bfseries
the code is quite effective and easy to use in Python. 

The main objective --- to find out the possibility
of accelerating programs implementing the tensor algorithms
written in Python using the tools of autogeneration code for the
GPU. \mdseries

* The relevance of research
The attractiveness of research is driven by several factors:
1) Tensor algorithms have begun to actively developed in recent years
2) Writing the GPU-code --- hard task, there is a need to explore the
   possibilities  of autogeneration of GPU-code
3) GPU computational power is superior multi-core CPU, GPU is more effective

Because the process of writing the GPU-code by hand is time-consuming
though effective, in computational problems  we would like to use the
following "ideal" way of writing it:
1) Use in dynamic languages ​​(Python)
2) Automatic parallelization of routine tasks (cycles), the generation
   of OpenCL / CUDA-code
3) High efficiency

* Model problem
As example of approximation algorithm let's consider algorithm of
constructing the canonical decomposition

It's needed to enter the following definitions

/Definition/\\
 Tensor A dimensionality $d$ is said to be a multidimensional array,
 which elements A(i_1,i_2,\ldots,i_d) have $d$ indeсes. $1 \leq i_k
 \leq n_k$; $n_k$ are named modal sizes (mods sizes)
    
 /Definition/\\
 The canonical decomposition of multidimensional array (/tensor/) is a
 representation of the form:

\begin{equation}\label{curs:eq1}
A(i_1,i_2,\ldots,i_d) = \sum_{\alpha=1}^r U_1(i_1,\alpha) U_2(i_2,\alpha) \ldots U_d(i_d,\alpha),
\end{equation}
where U_k are called /factors/ of the canonical decomposition, and $r$
--- canonical rank.

Equation \eqref{curs:eq1} is called main equation. More details about tensors
and their decompositions could be found in article \cite{kolda2009tensor}

** ALS algorithm
  Given a tensor $A$ with elements $A_{i_1 \ldots i_d}$. The problem
  is to find it canonical approach, namely to find such matrices
  $U_1,\ldots,U_d$

\begin{equation}\label{curs:caneq}
A_{i_1,\ldots,i_d} \approx  \sum_{\alpha=1}^r U_1(i_1,\alpha) U_2(i_2,\alpha) \ldots U_d(i_d,\alpha).
\end{equation}
The mathematical formulation of the problem is to solve the problem
\eqref{curs:caneq} in sense of least squares
#+begin_latex
\begin{align}
\sum_{i_1,\ldots,i_d} \Big(A(i_1,\ldots,i_d)-
\sum_{\alpha=1}^r U_1(i_1,\alpha) U_2(i_2,\alpha) \ldots
U_d(i_d,\alpha)\Big) ^2
\longrightarrow \min.
\end{align}
#+end_latex

We shall solve a variational problem of finding tensor approximation
using the ALS algorithm (Alternating Least Squares), detailed account
of which can be found in the article \cite{carroll1970analysis}. The
basic idea of ​​the algorithm is to capture all factors of canonical
decomposition except one, and seek the minimum of functional only on
it. By means of cycling permutation using already recieved factors
subsequent constructed until the required accuracy of approximation is
reached or until other stopping criteria do not work (exceeding the
maximum number of iterations, excess run-time).
*** Evaluation of complexity ALS algorithm and the possibility of parallel implementation

Suppose that a given tensor A has the dimensions of modes $n$ and rank $r$.

Simpliest programm for constructing each factor U_{i \alpha} could be
created using nested loops.Then the complexity of a right and left
parts of the system, respectively:

1) The complexity of calculating the left-hand side of the system for
   a single matrix U is proportional to $O(nr^2)$;
2) The complexity of calculating the right-hand side $O (n^3r)$;

that is at $n=512$ requires much time to calculate.Comparative
characteristics of ALS algorithm can be found in the article
\cite{faber2003recent}

\bfseries The main task of programming \mdseries could be formulated
\bfseries
1) Highlight the most time-consuming cycle
2) Parallelize it using the package loopy
\mdseries
* About Loopy package
** Installation
Loopy package nowadays has several dependencies. That packages should
be installed before the Loopy:
- gmpy [[https://code.google.com/p/gmpy/]]
- pyopencl [[ http://github.com/inducer/pyopencl]]
- pympolic [[http://github.com/inducer/pymbolic]]
- islpy [[http://github.com/inducer/islpy]]
- cgen [[http://github.com/inducer/cgen]]
Most of them could be downloaded with git. After the instalation of
packages Loopy could be installed and you could start work with
it. You may find Loopy  here: [[http://git.tiker.net/loopy.git]].

** Purpose and syntax of Loopy
Loopy package is designed to automatically generate OpenCL-code, wich
could be used on GPU. For using code auto-generation method (with help
of Loopy) an algorith initially should to be transformed to the
algorithm with nested loops (sequence of nested loops). The main
objective of this module is to "unroll" nested loops and it has a
possibility to convert loops of varying nesting. In operation Loopy
generates a computational kernel, which would be executed on
GPU. Here is an example of the kernel, which
use the basic functions pf the package:
#+begin_src python :exports code
def LU_solver(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[l,k,i,j,m]: 0<=l<r and 0<=k<n-1 and k+1<=i<n and 0<=j<n-1 and 0<=m<n-1-j}",
    
  ],
  [
  "bcopy[i,l] = bcopy[i,l]-bcopy[k,l]*LU[i,k] {id=lab1}",
  "bcopy[n-1-j,l]=bcopy[n-j-1,l]/LU[n-j-1,n-1-j] {id=l2, dep=lab1}",
  "bcopy[m,l]= bcopy[m,l]-bcopy[n-j-1,l]*LU[m,n-1-j] {id=l3, dep =l2}",
  "bcopy[0,l]=bcopy[0,l]/LU[0,0]{id=l4, dep=l2}",
  ],
  [
  lp.GlobalArg("LU", dtype, shape = "n, n" , order=order),
  lp.GlobalArg("bcopy", dtype, shape = "n, r" , order=order),
  lp.ValueArg("n", np.int64),
  lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", 1)
  knl = lp.split_iname(knl, "i", 32)
  knl = lp.split_iname(knl, "j", 32)
  knl = lp.split_iname(knl, "l", 32, outer_tag="g.0", inner_tag="l.0")

  print knl
  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl
#+end_src
The above code implements a system solution submitted in standard form
of LU-decomposition, algorithm is applied in a special way using
Loopy's the syntax.
** Kernel input parameter
To the input of the function implementing kernel the context of the
program is suplied. The conventional way of getting context is the
following:
#+begin_src python :exports code
plt = cl.get_platforms()
nvidia_plat = plt[1]
ctx = cl.Context(nvidia_plat.get_devices())
#+end_src
After the code execution to variable \bfseries ctx \mdseries the
context, corresponding to graphics card (in this case NVIDIA), would
be served.
** The inner elements of the kernel
The generation of the kernel into the variable \bfseries knl \mdseries
the function \bfseries make_ kernel \mdseries is engaged, on input of
which is supplied:
- Domain, in other words names of variables-iterators of loops with
  their boundary conditions as a string.
Loopy supports cycles with pre-unknown boundary conditions, variable
conditions
#+begin_src python :exports code
 "{[l,k,i,j,m]: 0<=l<r and 0<=k<n-1 and k+1<=i<n and 0<=j<n-1 and 0<=m<n-1-j}",
#+end_src
In example loop's variable are $l,k,i,j,m$, where $l \in [0,r)$ and
$r$ hasn't been defined nowhere before and would be determined during
execution from input parameters. $k$ being the iterator of the loop
enveloping invested in it cycle for $i$ is defined in the varying
range. Thus it is possible to construct
a broad class of algorithms that allow such implementation.
 
- Instructions to be executed (at least one), each of wich the label
  and dependencies  could be assigned with help of variable $id$ and
  $dep$. /Instruction $id = label1$ depends on instruction   $id =
  lab2$ if it should be performed after instruction/ $lab2$.
Exaple instruction:
#+begin_src python :exports code
 "bcopy[i,l] = bcopy[i,l]-bcopy[k,l]*LU[i,k] {id=lab1}",
 "bcopy[n-1-j,l]=bcopy[n-j-1,l]/LU[n-j-1,n-1-j] {id=l2, dep=lab1}",
#+end_src

- Arguments, which include input parameters, constants, output
  parameters.
Every parameter should have type, size (possible to specify character
in "implicit" as well as the explicit numerical or in the form of
variable (which should have been previously defined))
 
Argument example:
#+begin_src python :exports code
lp.GlobalArg("LU", dtype, shape = "n, n" , order=order),
#+end_src
- Additional parameters as an admission, approximate dimension or
  size.
Examples may be found in  $test$ directory of Loopy package.
** Partition of computational grid
After the kernel is written, it is necessary to specify the way in
which computational grid for this kernel has to be splitted (how to
split loops). This deals with the function " split_ iname":
#+begin_src python :exports code
 knl = lp.split_iname(knl, "l", 32, outer_tag="g.0", inner_tag="l.0")
#+end_src
First parameter --- kernel, loops of which should be splitted. Next
--- name of counter variable, further indicated the size of how loop
should be splitted (usually 16 or 32, the partition is recommended,
but may any other). In the end optional parameters of inner and outer
work groups  are specified. 
*** About the choice of parameters of partition
Unfortunately there is no universal algorithm how to choose the
partition. But at the same time the quallity of programm strongly
depends on choice of "outer_ tag" and "inner_ tag". There are some
basic rules that  will help to go much of the way, like "always make
sure that local axis 0 has stride 1", but doing this in a way that
will get good performance for complicated memory access/communication
patterns is just difficult, and hasn't been  successfully and robustly
automated. For Loopy user that means that better choose standard
partition and experimentally find the best one.
* About kernel call
** The location of arrays
Once the kernel is written, partition arranged, the kernel can be
used. But before it some preparations recommended to be done: all
parameters (arrays, tensors) move to the device (for saving
significant time). To do this, perform a series of commands. 
- Create a queue
#+begin_src python :exports code
queue = cl.CommandQueue(ctx,properties=cl.command_queue_properties.PROFILING_ENABLE)
#+end_src
- By special command  cl.array_ to device(queue, variable) move
   object  variable to device
#+begin_src python :exports code
u2=cl.array.to_device(queue,u)
#+end_src
To get results back (u as a numpy.array) get() gives you numpy.array.
#+begin_src python :exports code
numpy_array_u2 = u2.get()
#+end_src
\bfseries It is important that all arrays have a explicitly defined
type \mdseries
 
The kernel call is simalar to a function call. But before the call
some commands need to be executed:
- Create a queue --- "queue". \bfseries The queue must be unique! \mdseries
- Create a vocabulary of parameters --- "parameters". Output
  parameters may be in it or not. 
- Compile the kernel. The kernel may be compiled once and saved in
  a special variable to use.
- Call the compiled kernel with parameters "queue" and "parameters"
Here is an example of kernel call:
#+begin_src python :exports code
cknl_r_U = lp.CompiledKernel(ctx, knl_r_U)
parameters={"a":a2,"v":v2,"w":w2,"n":n,"r":r,"f":prav}
evt=cknl_r_U(queue, **parameters)[0]
#evt,(f)= cknl_r_U(queue, **parameters) This method uses the shipment
#and  therefore not very good
evt.wait()
#+end_src
* Platforms
During the implementation of a paper the following computational
platforms were used:
- Mobile GPU NVIDIA
- Processor Intel Core i5
- Cluster INM RAS tesla
We include information about the cluster INM (as most of the
experiments performed on it)

|                                         Device Tesla C2070                            |   |
|                                         |                                               |   |
| CL_ DEVICE_ NAME:                       | Tesla C2070                                   |   |
| CL_ DEVICE_ VENDOR:                     | NVIDIA Corporation                            |   |
| CL_ DRIVER_ VERSION:                    | 304.54                                        |   |
| CL_ DEVICE_ VERSION:                    | OpenCL 1.1 CUDA                               |   |
| CL_ DEVICE_ OPENCL_ C_ VERSION:         | OpenCL C 1.1                                  |   |
| CL_ DEVICE_ TYPE:                       | CL_ DEVICE_ TYPE_ GPU                         |   |
| CL_ DEVICE_ MAX_  COMPUTE_UNITS:        | 14                                            |   |
| CL_ DEVICE_ MAX_ WORK_ ITEM_ DIMENSIONS: | 3                                             |   |
| CL_ DEVICE_ MAX_ WORK_ ITEM_ SIZES:     | 1024 / 1024 / 64                              |   |
| CL_ DEVICE_ MAX_ WORK_ GROUP_ SIZE:     | 1024                                          |   |
| CL_ DEVICE_ MAX_ CLOCK_ FREQUENCY:      | 1147 MHz                                      |   |
| CL_ DEVICE_ ADDRESS_ BITS:              | 32                                            |   |
| CL_ DEVICE_ MAX_ MEM_ ALLOC_ SIZE:      | 1343 MByte                                    |   |
| CL_ DEVICE_ GLOBAL_ MEM_ SIZE:          | 5375 MByte                                    |   |
| CL_ DEVICE_ ERROR_ CORRECTION_ SUPPORT: | yes                                           |   |
| CL_ DEVICE_ LOCAL_ MEM_ TYPE:           | local                                         |   |
| CL_ DEVICE_ LOCAL_ MEM_ SIZE:           | 48 KByte                                      |   |
| CL_ DEVICE_ MAX_ CONSTANT_ BUFFER_ SIZE: | 64 KByte                                      |   |
| CL_ DEVICE_ IMAGE_ SUPPORT:             | 1                                             |   |
| CL_ DEVICE_ MAX_ READ_ IMAGE_ ARGS:     | 128                                           |   |
| CL_ DEVICE_ MAX_ WRITE_ IMAGE_ ARGS:    | 8                                             |   |

* Numerical experiments
During research seveeral algorithms have been implemented: algorithm
of calculating the right-hand side, algorithm of solving SLAE using
LU-decomposition, ALS algorithm. For each one the OpenCL realisation
has been acquired and executed on listed above platforms. The
experiments were carried out with the tensor of dimension $d=3$
(three-dimensional tensor) and various mod's sizes $n$ and rank
$r$. During experiments on Tesla the following results were recieved:
For fixed rank $r=3$ and modal sizes $n$ investigated as execution speed
of every kernel as ALS algoritm entirely. It should be noted
ALS-algorithm does not guaratee the convergence, therefore we will
only specify the runtime of one iteration.
Here is the table with time of execution:

| size n                      |        128 |        256 |      512 |      756 |
| right-hand side computation |   0.013803 |    0.08674 |  0.65225 |  0.92513 |
| left-hand side computation  | 0.00035595 |  0.0004210 | 0.000552 | 0.000673 |
| solving SLAE                | 0.00025391 | 0.00025510 | 0.000256 | 0.000256 |
| LU-decomposition            | 0.00024890 |  0.0002851 |  0.00035 | 0.000391 |
| one iteration time          |   0.026740 |     0.1834 |  1.08289 |  1.92985 |
|                             |            |            |          |          |

We also present a table with the execution time of one iteration of
the program, calculate the right side, depending on the rank $r$ and
a fixed dimension of the tensor $n = 128$

| rank r                      |       3 |      6 |     10 |     20 |
| right-hand side computation | 0.01380 | 0.0152 | 0.0162 | 0.0184 |
| one iteration time          | 0.04326 | 0.0437 | 0.0468 | 0.0556 |
|                             |         |        |        |        |

For clarity, we also construct graphs of behavior computation time of
the right-hand side on CPU, mobile GPU and Tesla:

#+begin_center
#+attr_latex: placement=[H]
#+ATTR_LaTeX: width=15cm
#+caption: The dependence of the execution time of one iteration of the size $ n $. Blue line on the graph corresponds to a mobile GPU, the green CPU, red Tesla. Clippings lines mean that the tensor bigger is not located in the memory.

[[file:plot.pdf]]
#+end_center


* Conclusion
Means of of automatic code generation is convenient to use for
parallelization of tensor algorithms using computational capacity of
GPUs.  The acceleration of the program is expected substantial, both
because of the high performance of GPU, and because of the structure
of the algorithm. Tensor algorithms are widely claimed, and the
creation of an effective and rapid implementation is a priority task, while
the possibility of auto-generating code on the GPU can create
an implementation fast.

On the example of ALS algorithm the ability to
automatically parallelize the algorithms using tensor package loo.py
was demonstrated. A major acceleration of ALS was recieved, a
method of automatically parallelizing tensor algorithms was
investigated.   

* Appendix
In Appendix Python kernels which were used during the work are
presented. The names of kernels should help to understand which
algorithm they implement. 

#+begin_src python :exports code

#def LU_decomposition(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    "{[k,i]: 0<=k<n-1 and k+1<=i<n}",
    "{[j,l]: 0<=k<n-1 and k+1<=j,l<n}",
  ],
  [
  "syst[i,k] = syst[i,k]/syst[k,k] {id=lab1}",
  "syst[l,j]= syst[l,j] - syst[l,k]*syst[k,j] {dep=lab1}",
  ],
  [
  lp.GlobalArg("syst", dtype, shape = "n, n" , order=order),
  lp.ValueArg("n", np.int32),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", n)
  knl = lp.split_iname(knl, "i", 32)
  knl = lp.split_iname(knl, "j", 32)
  knl = lp.split_iname(knl, "l", 32)

#  print knl
#  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl

def LU_solver(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[l,k,i,j,m]: 0<=l<r and 0<=k<n-1 and k+1<=i<n and 0<=j<n-1 and 0<=m<n-1-j}",
    
  ],
  [
  "bcopy[i,l] = bcopy[i,l]-bcopy[k,l]*LU[i,k] {id=lab1}",
  "bcopy[n-1-j,l]=bcopy[n-j-1,l]/LU[n-j-1,n-1-j] {id=l2, dep=lab1}",
  "bcopy[m,l]= bcopy[m,l]-bcopy[n-j-1,l]*LU[m,n-1-j] {id=l3, dep =l2}",
  "bcopy[0,l]=bcopy[0,l]/LU[0,0]{id=l4, dep=l2}",
  ],
  [
  lp.GlobalArg("LU", dtype, shape = "n, n" , order=order),
  lp.GlobalArg("bcopy", dtype, shape = "n, r" , order=order),
  lp.ValueArg("n", np.int64),
  lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", 1)
  knl = lp.split_iname(knl, "i", 32)
  knl = lp.split_iname(knl, "j", 32)
  knl = lp.split_iname(knl, "l", 32, outer_tag="g.0", inner_tag="l.0")

#  print knl
#  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl
def Prav_U(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,j,k,alpha]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "f[alpha,i]=sum((j,k), a[i,j,k]*v[alpha,j]*w[alpha,k])",
  ],
  [
    lp.GlobalArg("a", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("f", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "i", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 1, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "k", 16)
  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl


def Prav_V(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,j,k,alpha]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "f[alpha,j]=sum((k,i), a[i,j,k]*w[alpha, k]*u[alpha, i])",
  ],
  [
    lp.GlobalArg("a", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("f", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "j", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "i", 16)
  knl = lp.split_iname(knl, "k", 16) 
   
  return knl

def Prav_W(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,j,k,alpha]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "f[alpha,k]=sum((i,j), a[i,j,k]*u[alpha, i]*v[alpha, j])",
  ],
  [
    lp.GlobalArg("a", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("f", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "i", 16) 
  

  return knl

def left_U(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[j,k,alpha,alpha1]: 0<=alpha,alpha1<r and 0<=j,k<n}",
    
  ],
  [
    "l[alpha,alpha1]=sum((j), v[alpha,j]*v[alpha1,j])*sum((k),w[alpha,k]*w[alpha1,k])",
  ],
  [

    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("l", dtype, shape="r, r", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "alpha1", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "k", 16)
  
  return knl

def left_V(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,k,alpha,alpha1]: 0<=alpha,alpha1<r and 0<=i,k<n}",
    
  ],
  [
    "l[alpha,alpha1]=sum((i), u[alpha,i]*u[alpha1,i])*sum((k),w[alpha,k]*w[alpha1,k])",
  ],
  [

    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("l", dtype, shape="r, r", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "alpha1", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "i", 16)
  knl = lp.split_iname(knl, "k", 16)
  
  return knl

def left_W(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[j,i,alpha,alpha1]: 0<=alpha,alpha1<r and 0<=j,i<n}",
    
  ],
  [
    "l[alpha,alpha1]=sum((i), u[alpha,i]*u[alpha1,i])*sum((j),v[alpha,j]*v[alpha1,j])",
  ],
  [

    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("l", dtype, shape="r, r", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "alpha1", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "i", 16)
  
  return knl

def get_tensor(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[j,i,alpha,k]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "res[i,j,k]=sum((alpha), u[alpha,i]*v[alpha,j]*w[alpha,k])",
  ],
  [
    lp.GlobalArg("res", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int32),
    lp.ValueArg("r", np.int32),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "i", 8,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "j", 8, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "alpha", 2)
  knl = lp.split_iname(knl, "k", 8, outer_tag="g.2", inner_tag="l.2" )
  
  return knl
#+end_src
 
\bibliography{cursov}
\bibliographystyle{plain}

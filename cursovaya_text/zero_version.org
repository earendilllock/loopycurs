#+STARTUP: overview
#+STARTUP: hidestars
#+OPTIONS: LaTeX:t
#+OPTIONS: toc:nil
#+LaTeX_CLASS: per-file-class

#+TITLE: Tensor approximation on GPU using Loo.py
#+AUTHOR: M. A. Kuznetsov, I. V. Oseledets, A. Kloeckner
#+DATE: \today
* LATEX OPTIONS 						   :noexport:
#+OPTIONS: toc:nil
** Packages
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{placeins}
#+LATEX_HEADER: \usepackage[T2A]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amsfonts,amsmath,amssymb}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{algorithmic} \usepackage[ruled]{algorithm}
#+LATEX_HEADER: \usepackage[unicode=true,plainpages=false]{hyperref}
#+LATEX_HEADER: \hypersetup{colorlinks=true,linkcolor=magenta,anchorcolor=magenta,urlcolor=blue,citecolor=blue}
** User-defined symbols
#+LATEX_HEADER: \def\A{\mathbf{A}}
#+LATEX_HEADER: \def\V{\mathbf{V}}
#+LATEX_HEADER: \def\B{\mathbf{B}}
#+LATEX_HEADER: \def\C{\mathbf{C}}
#+LATEX_HEADER: \usepackage{minted}
** Geometry
#+LATEX_HEADER: \usepackage[left=2.5cm,top=2cm,right=2cm,bottom=2cm,a4paper]{geometry}







* Introduction
To solve modern computer tasks it is neсessary to use huge
computational power. One of the most effective computational tools are
graphics processors, however, despite the development of design tools,
creating GPU-code takes a lot of time. In itself, programming on GPU
is a difficult task, so would like to have the technology of
automatic parallelization, but they often lose the "manual"
programming. It is possible to single out a class of problems, which
is subject to automatic generation of effective GPU-code, and the
resulting code could be used in dynamical languages (such as Python
--- [[http://www.python.org]]). Until recently such a convenient tool was
not, but they've started to develop rapidly. We take as a basis
project loopy ([[http://git.tiker.net/loopy.git]]), designed by Andreas
Kloekner ([[http://mathema.tician.de/aboutme/]]), that could generate
OpenCL Python-modules for small, but
laborious cycles.


* Purpose of the work
The purpose of this work is study applicability of the GPU code
autogeneration based on the loo.py package for the problem of tensor
approximation. By tensors we mean $d$-dimensional arrays and in this
note we focus only on three-dimensional tensors. It is well known,
that working with multidimensional arrays is quite tedious, and the
parallel versions can be quite handy. However, not much is known about
the GPU implementations of the basic tensor algorithms
\cite{ballard-gpu-2011}. The problem is that typical GPU programming
is still time-consuming and platform-dependent, thus new tools can be
very handy. 

The Python ecosystem is a perfect fit for fast GPU programming. This
seems to be strange at the first glance, given a well-known slow
execution of the Python code in comparison with the compiled
languages. The OpenCL framework can be accessed from Python by using
pyOpenCL package ([[http://mathema.tician.de/software/pyopencl]]). It
handles several basic tasks including a natural syntax for array
operations. The kernel code is still to be programmed ``by
hand''. Even simple tasks (like matrix-by-matrix product), when
implemented manually, lead to hundreds of lines of code that takes
into account, for example, the boundary cases in the nested
loops. Some part of the work can be automated. This is what the loo.py
packages is for: it is suited for a subclass of kernels that can be
written as a loops. The idea of parallelization (how to split the
loop into grids, which indices use as the first ones) comes from the
user: it is a crucial points since many of these parameters are
machine-dependent. The technical tasks are handled by the loo.py
package which gives, as an output, the code of the OpenCL kernel. The
code is then plugged in into the pyOpenCL package and it makes the
running program.

The idea of applying loo.py to the tensor approximation problem is
very natural. In this paper we consider the basic algorithm in the
canonical approximation: the alternating least squares problem. The
computational core of the algorithm consists in the computation of
multidimensional sums and in the solution of linear systems of
equations. In total, the final algorithm required 13 different OpenCL
kernels, each taking around 100 lines of code. These kernel functions
were written in the loo.py syntax.
The main objective --- to find out the possibility
of accelerating programs implementing the tensor algorithms
written in Python using the tools of autogeneration code for the
GPU.

* The problem
As example of approximation algorithm let's consider algorithm of
constructing the canonical decomposition

It's needed to enter the following definitions

/Definition/\\
 Tensor A dimensionality $d$ is said to be a multidimensional array,
 which elements A(i_1,i_2,\ldots,i_d) have $d$ indeсes. $1 \leq i_k
 \leq n_k$; $n_k$ are called mode sizes
    
 /Definition/\\
 The canonical decomposition of multidimensional array (/tensor/) is a
 representation of the form:

\begin{equation}\label{curs:eq1}
A(i_1,i_2,\ldots,i_d) = \sum_{\alpha=1}^r U_1(i_1,\alpha) U_2(i_2,\alpha) \ldots U_d(i_d,\alpha),
\end{equation}
where U_k are called /factors/ of the canonical decomposition, and $r$
--- canonical rank.

The equation \eqref{curs:eq1} is called main equation. More details about tensors
and their decompositions could be found in article \cite{kolda-review-2009}

** ALS algorithm
  Given a tensor $A$ with elements $A_{i_1 \ldots i_d}$. The problem
  is to find it canonical approach, namely to find such matrices
  $U_1,\ldots,U_d$

\begin{equation}\label{curs:caneq}
A_{i_1,\ldots,i_d} \approx  \sum_{\alpha=1}^r U_1(i_1,\alpha) U_2(i_2,\alpha) \ldots U_d(i_d,\alpha).
\end{equation}
The mathematical formulation of the problem is to solve the problem
\eqref{curs:caneq} in sense of least squares
#+begin_latex
\begin{align}
\sum_{i_1,\ldots,i_d} \Big(A(i_1,\ldots,i_d)-
\sum_{\alpha=1}^r U_1(i_1,\alpha) U_2(i_2,\alpha) \ldots
U_d(i_d,\alpha)\Big) ^2
\longrightarrow \min.
\end{align}
#+end_latex

We shall solve a variational problem of finding tensor approximation
using the ALS algorithm (Alternating Least Squares) \cite{als1, als2, kolda-review-2009}. The
basic idea of the algorithm is to fix all factors of the canonical
decomposition except one. This gives a quadratic optimization
problem. After that we optimize over another factor and so on. It is
guaranteed that the error does not increase during the algorithm, but
the convergence can be quite slow (so-called swamp behaviour). The
number of iterations to reach the solution may be millions. 
*** Evaluation of complexity ALS algorithm and the possibility of parallel implementation

Suppose that a given tensor A has the dimensions of modes $n$ and rank $r$.

Simpliest programm for constructing each factor U_{i \alpha} could be
created using nested loops.Then the complexity of a right and left
parts of the system, respectively:

1) The complexity of calculating the left-hand side of the system for
   a single matrix U is proportional to $O(nr^2)$;
2) The complexity of calculating the right-hand side $O (n^3r)$;

that is at $n=512$ requires much time to calculate.Comparative
characteristics of ALS algorithm can be found in the article
\cite{faber2003recent}

\bfseries The main task of programming \mdseries could be formulated
\bfseries
1) Highlight the most time-consuming cycle
2) Parallelize it using the package loopy
\mdseries
* About Loopy package :noexport:
** Installation
Loopy package nowadays has several dependencies. That packages should
be installed before the Loopy:
- gmpy [[https://code.google.com/p/gmpy/]]
- pyopencl [[ http://github.com/inducer/pyopencl]]
- pympolic [[http://github.com/inducer/pymbolic]]
- islpy [[http://github.com/inducer/islpy]]
- cgen [[http://github.com/inducer/cgen]]
Most of them could be downloaded with git. After the instalation of
packages Loopy could be installed and you could start work with
it. You may find Loopy  here: [[http://git.tiker.net/loopy.git]].

** Purpose and syntax of Loopy
Loopy package is designed to automatically generate OpenCL-code, wich
could be used on GPU. For using code auto-generation method (with help
of Loopy) an algorith initially should to be transformed to the
algorithm with nested loops (sequence of nested loops). The main
objective of this module is to "unroll" nested loops and it has a
possibility to convert loops of varying nesting. In operation Loopy
generates a computational kernel, which would be executed on
GPU. Here is an example of the kernel, which
use the basic functions pf the package:
#+begin_src python :exports code
def LU_solver(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[l,k,i,j,m]: 0<=l<r and 0<=k<n-1 and k+1<=i<n and 0<=j<n-1 and 0<=m<n-1-j}",
    
  ],
  [
  "bcopy[i,l] = bcopy[i,l]-bcopy[k,l]*LU[i,k] {id=lab1}",
  "bcopy[n-1-j,l]=bcopy[n-j-1,l]/LU[n-j-1,n-1-j] {id=l2, dep=lab1}",
  "bcopy[m,l]= bcopy[m,l]-bcopy[n-j-1,l]*LU[m,n-1-j] {id=l3, dep =l2}",
  "bcopy[0,l]=bcopy[0,l]/LU[0,0]{id=l4, dep=l2}",
  ],
  [
  lp.GlobalArg("LU", dtype, shape = "n, n" , order=order),
  lp.GlobalArg("bcopy", dtype, shape = "n, r" , order=order),
  lp.ValueArg("n", np.int64),
  lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", 1)
  knl = lp.split_iname(knl, "i", 32)
  knl = lp.split_iname(knl, "j", 32)
  knl = lp.split_iname(knl, "l", 32, outer_tag="g.0", inner_tag="l.0")

  print knl
  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl
#+end_src
The above code implements a system solution submitted in standard form
of LU-decomposition, algorithm is applied in a special way using
Loopy's the syntax.
** Kernel input parameter
To the input of the function implementing kernel the context of the
program is suplied. The conventional way of getting context is the
following:
#+begin_src python :exports code
plt = cl.get_platforms()
nvidia_plat = plt[1]
ctx = cl.Context(nvidia_plat.get_devices())
#+end_src
After the code execution to variable \bfseries ctx \mdseries the
context, corresponding to graphics card (in this case NVIDIA), would
be served.
** The inner elements of the kernel
The generation of the kernel into the variable \bfseries knl \mdseries
the function \bfseries make_ kernel \mdseries is engaged, on input of
which is supplied:
- Domain, in other words names of variables-iterators of loops with
  their boundary conditions as a string.
Loopy supports cycles with pre-unknown boundary conditions, variable
conditions
#+begin_src python :exports code
 "{[l,k,i,j,m]: 0<=l<r and 0<=k<n-1 and k+1<=i<n and 0<=j<n-1 and 0<=m<n-1-j}",
#+end_src
In example loop's variable are $l,k,i,j,m$, where $l \in [0,r)$ and
$r$ hasn't been defined nowhere before and would be determined during
execution from input parameters. $k$ being the iterator of the loop
enveloping invested in it cycle for $i$ is defined in the varying
range. Thus it is possible to construct
a broad class of algorithms that allow such implementation.
 
- Instructions to be executed (at least one), each of wich the label
  and dependencies  could be assigned with help of variable $id$ and
  $dep$. /Instruction $id = label1$ depends on instruction   $id =
  lab2$ if it should be performed after instruction/ $lab2$.
Exaple instruction:
#+begin_src python :exports code
 "bcopy[i,l] = bcopy[i,l]-bcopy[k,l]*LU[i,k] {id=lab1}",
 "bcopy[n-1-j,l]=bcopy[n-j-1,l]/LU[n-j-1,n-1-j] {id=l2, dep=lab1}",
#+end_src

- Arguments, which include input parameters, constants, output
  parameters.
Every parameter should have type, size (possible to specify character
in "implicit" as well as the explicit numerical or in the form of
variable (which should have been previously defined))
 
Argument example:
#+begin_src python :exports code
lp.GlobalArg("LU", dtype, shape = "n, n" , order=order),
#+end_src
- Additional parameters as an admission, approximate dimension or
  size.
Examples may be found in  $test$ directory of Loopy package.
** Partition of computational grid
After the kernel is written, it is necessary to specify the way in
which computational grid for this kernel has to be splitted (how to
split loops). This deals with the function " split_ iname":
#+begin_src python :exports code
 knl = lp.split_iname(knl, "l", 32, outer_tag="g.0", inner_tag="l.0")
#+end_src
First parameter --- kernel, loops of which should be splitted. Next
--- name of counter variable, further indicated the size of how loop
should be splitted (usually 16 or 32, the partition is recommended,
but may any other). In the end optional parameters of inner and outer
work groups  are specified. 
*** About the choice of parameters of partition
Unfortunately there is no universal algorithm how to choose the
partition. But at the same time the quallity of programm strongly
depends on choice of "outer_ tag" and "inner_ tag". There are some
basic rules that  will help to go much of the way, like "always make
sure that local axis 0 has stride 1", but doing this in a way that
will get good performance for complicated memory access/communication
patterns is just difficult, and hasn't been  successfully and robustly
automated. For Loopy user that means that better choose standard
partition and experimentally find the best one. 
* About kernel call :noexport:
** The location of arrays
Once the kernel is written, partition arranged, the kernel can be
used. But before it some preparations recommended to be done: all
parameters (arrays, tensors) move to the device (for saving
significant time). To do this, perform a series of commands. 
- Create a queue
#+begin_src python :exports code
queue = cl.CommandQueue(ctx,properties=cl.command_queue_properties.PROFILING_ENABLE)
#+end_src
- By special command  cl.array_ to device(queue, variable) move
   object  variable to device
#+begin_src python :exports code
u2=cl.array.to_device(queue,u)
#+end_src
To get results back (u as a numpy.array) get() gives you numpy.array.
#+begin_src python :exports code
numpy_array_u2 = u2.get()
#+end_src
\bfseries It is important that all arrays have a explicitly defined
type \mdseries
 
The kernel call is simalar to a function call. But before the call
some commands need to be executed:
- Create a queue --- "queue". \bfseries The queue must be unique! \mdseries
- Create a vocabulary of parameters --- "parameters". Output
  parameters may be in it or not. 
- Compile the kernel. The kernel may be compiled once and saved in
  a special variable to use.
- Call the compiled kernel with parameters "queue" and "parameters"
Here is an example of kernel call:
#+begin_src python :exports code
cknl_r_U = lp.CompiledKernel(ctx, knl_r_U)
parameters={"a":a2,"v":v2,"w":w2,"n":n,"r":r,"f":prav}
evt=cknl_r_U(queue, **parameters)[0]
#evt,(f)= cknl_r_U(queue, **parameters) This method uses the shipment
#and  therefore not very good
evt.wait()
#+end_src
* Platforms
During the implementation of a paper the following computational
platforms were used:
- Mobile GPU NVIDIA
- Processor Intel Core i5
- Cluster INM RAS tesla
We include information about the cluster INM (as most of the
experiments performed on it)

|                                         Device Tesla C2070                            |   |
|                                         |                                               |   |
| CL_ DEVICE_ NAME:                       | Tesla C2070                                   |   |
| CL_ DEVICE_ VENDOR:                     | NVIDIA Corporation                            |   |
| CL_ DRIVER_ VERSION:                    | 304.54                                        |   |
| CL_ DEVICE_ VERSION:                    | OpenCL 1.1 CUDA                               |   |
| CL_ DEVICE_ OPENCL_ C_ VERSION:         | OpenCL C 1.1                                  |   |
| CL_ DEVICE_ TYPE:                       | CL_ DEVICE_ TYPE_ GPU                         |   |
| CL_ DEVICE_ MAX_  COMPUTE_UNITS:        | 14                                            |   |
| CL_ DEVICE_ MAX_ WORK_ ITEM_ DIMENSIONS: | 3                                             |   |
| CL_ DEVICE_ MAX_ WORK_ ITEM_ SIZES:     | 1024 / 1024 / 64                              |   |
| CL_ DEVICE_ MAX_ WORK_ GROUP_ SIZE:     | 1024                                          |   |
| CL_ DEVICE_ MAX_ CLOCK_ FREQUENCY:      | 1147 MHz                                      |   |
| CL_ DEVICE_ ADDRESS_ BITS:              | 32                                            |   |
| CL_ DEVICE_ MAX_ MEM_ ALLOC_ SIZE:      | 1343 MByte                                    |   |
| CL_ DEVICE_ GLOBAL_ MEM_ SIZE:          | 5375 MByte                                    |   |
| CL_ DEVICE_ ERROR_ CORRECTION_ SUPPORT: | yes                                           |   |
| CL_ DEVICE_ LOCAL_ MEM_ TYPE:           | local                                         |   |
| CL_ DEVICE_ LOCAL_ MEM_ SIZE:           | 48 KByte                                      |   |
| CL_ DEVICE_ MAX_ CONSTANT_ BUFFER_ SIZE: | 64 KByte                                      |   |
| CL_ DEVICE_ IMAGE_ SUPPORT:             | 1                                             |   |
| CL_ DEVICE_ MAX_ READ_ IMAGE_ ARGS:     | 128                                           |   |
| CL_ DEVICE_ MAX_ WRITE_ IMAGE_ ARGS:    | 8                                             |   |

* Numerical experiments
The following kernel functions were implemented in loo.py: calculation
of the right-hand side, solution of linear system (LU decomposition +
backward substitution). 
experiments were carried out with the tensor of dimension $d=3$
(three-dimensional tensor) and various mode sizes $n$ and rank
$r$. The time for one iteration for varying $n$ and fixed $r$ is given
in the Table~\ref{fixed-r}. The Table~\ref{fixed-n} presents timings
for fixed $n$ and varying $r$.

| size n                      |        128 |        256 |      512 |      756 |
| right-hand side computation |   0.013803 |    0.08674 |  0.65225 |  0.92513 |
| left-hand side computation  | 0.00035595 |  0.0004210 | 0.000552 | 0.000673 |
| solving linear system       | 0.00025391 | 0.00025510 | 0.000256 | 0.000256 |
| LU-decomposition            | 0.00024890 |  0.0002851 |  0.00035 | 0.000391 |
| one iteration time          |   0.026740 |     0.1834 |  1.08289 |  1.92985 |
|                             |            |            |          |          |


| rank r                      |       3 |      6 |     10 |     20 |
| right-hand side computation | 0.01380 | 0.0152 | 0.0162 | 0.0184 |
| one iteration time          | 0.04326 | 0.0437 | 0.0468 | 0.0556 |
|                             |         |        |        |        |

The comparison of different platforms is done on Figure~\ref{comp}

#+begin_center
#+name: comp
#+attr_latex: placement=[H]
#+ATTR_LaTeX: width=15cm
#+caption: The dependence of the execution time of one iteration of the size $ n $. Blue line on the graph corresponds to a mobile GPU, the green CPU, red Tesla. Clippings lines mean that the tensor bigger is not located in the memory.

[[file:plot.pdf]]
#+end_center


* Conclusion
The loo.py package was successfully used for the GPU implementation of
the ALS algorithm. The flexibility of the packages allows for fast
algorithm prototyping and implementation on different platforms. The
GPU version is faster than the CPU implementation. There are several
possible variants how to achieve maximum possible efficiency,
including the improvement of data layout and using local memory. Even
without that, the results are quite promising and can be extended
without difficulties to other tensor algorithms.  

* Appendix
/Here we can put the reference to the repo/

#+begin_src python :exports code

#def LU_decomposition(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    "{[k,i]: 0<=k<n-1 and k+1<=i<n}",
    "{[j,l]: 0<=k<n-1 and k+1<=j,l<n}",
  ],
  [
  "syst[i,k] = syst[i,k]/syst[k,k] {id=lab1}",
  "syst[l,j]= syst[l,j] - syst[l,k]*syst[k,j] {dep=lab1}",
  ],
  [
  lp.GlobalArg("syst", dtype, shape = "n, n" , order=order),
  lp.ValueArg("n", np.int32),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", n)
  knl = lp.split_iname(knl, "i", 32)
  knl = lp.split_iname(knl, "j", 32)
  knl = lp.split_iname(knl, "l", 32)

#  print knl
#  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl

def LU_solver(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[l,k,i,j,m]: 0<=l<r and 0<=k<n-1 and k+1<=i<n and 0<=j<n-1 and 0<=m<n-1-j}",
    
  ],
  [
  "bcopy[i,l] = bcopy[i,l]-bcopy[k,l]*LU[i,k] {id=lab1}",
  "bcopy[n-1-j,l]=bcopy[n-j-1,l]/LU[n-j-1,n-1-j] {id=l2, dep=lab1}",
  "bcopy[m,l]= bcopy[m,l]-bcopy[n-j-1,l]*LU[m,n-1-j] {id=l3, dep =l2}",
  "bcopy[0,l]=bcopy[0,l]/LU[0,0]{id=l4, dep=l2}",
  ],
  [
  lp.GlobalArg("LU", dtype, shape = "n, n" , order=order),
  lp.GlobalArg("bcopy", dtype, shape = "n, r" , order=order),
  lp.ValueArg("n", np.int64),
  lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", 1)
  knl = lp.split_iname(knl, "i", 32)
  knl = lp.split_iname(knl, "j", 32)
  knl = lp.split_iname(knl, "l", 32, outer_tag="g.0", inner_tag="l.0")

#  print knl
#  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl
def Prav_U(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,j,k,alpha]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "f[alpha,i]=sum((j,k), a[i,j,k]*v[alpha,j]*w[alpha,k])",
  ],
  [
    lp.GlobalArg("a", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("f", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "i", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 1, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "k", 16)
  print lp.CompiledKernel(ctx, knl).get_highlighted_code()   
  return knl


def Prav_V(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,j,k,alpha]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "f[alpha,j]=sum((k,i), a[i,j,k]*w[alpha, k]*u[alpha, i])",
  ],
  [
    lp.GlobalArg("a", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("f", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "j", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "i", 16)
  knl = lp.split_iname(knl, "k", 16) 
   
  return knl

def Prav_W(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,j,k,alpha]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "f[alpha,k]=sum((i,j), a[i,j,k]*u[alpha, i]*v[alpha, j])",
  ],
  [
    lp.GlobalArg("a", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("f", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "k", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "i", 16) 
  

  return knl

def left_U(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[j,k,alpha,alpha1]: 0<=alpha,alpha1<r and 0<=j,k<n}",
    
  ],
  [
    "l[alpha,alpha1]=sum((j), v[alpha,j]*v[alpha1,j])*sum((k),w[alpha,k]*w[alpha1,k])",
  ],
  [

    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("l", dtype, shape="r, r", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "alpha1", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "k", 16)
  
  return knl

def left_V(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[i,k,alpha,alpha1]: 0<=alpha,alpha1<r and 0<=i,k<n}",
    
  ],
  [
    "l[alpha,alpha1]=sum((i), u[alpha,i]*u[alpha1,i])*sum((k),w[alpha,k]*w[alpha1,k])",
  ],
  [

    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.GlobalArg("l", dtype, shape="r, r", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "alpha1", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "i", 16)
  knl = lp.split_iname(knl, "k", 16)
  
  return knl

def left_W(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[j,i,alpha,alpha1]: 0<=alpha,alpha1<r and 0<=j,i<n}",
    
  ],
  [
    "l[alpha,alpha1]=sum((i), u[alpha,i]*u[alpha1,i])*sum((j),v[alpha,j]*v[alpha1,j])",
  ],
  [

    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("l", dtype, shape="r, r", order=order),
    lp.ValueArg("n", np.int64),
    lp.ValueArg("r", np.int64),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "alpha1", 16,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "alpha", 3, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "j", 16)
  knl = lp.split_iname(knl, "i", 16)
  
  return knl

def get_tensor(ctx):
  order='C'
  dtype = np.float32
  knl = lp.make_kernel(ctx.devices[0], 
  [
    
    "{[j,i,alpha,k]: 0<=alpha<r and 0<=i,j,k<n}",
    
  ],
  [
    "res[i,j,k]=sum((alpha), u[alpha,i]*v[alpha,j]*w[alpha,k])",
  ],
  [
    lp.GlobalArg("res", dtype, shape="n, n, n", order=order),
    lp.GlobalArg("v", dtype, shape="r, n", order=order),
    lp.GlobalArg("u", dtype, shape="r, n", order=order),
    lp.GlobalArg("w", dtype, shape="r, n", order=order),
    lp.ValueArg("n", np.int32),
    lp.ValueArg("r", np.int32),
  ],
  assumptions="n>=1")
  knl = lp.split_iname(knl, "i", 8,outer_tag="g.0", inner_tag="l.0")
  knl = lp.split_iname(knl, "j", 8, outer_tag="g.1", inner_tag="l.1")
  knl = lp.split_iname(knl, "alpha", 2)
  knl = lp.split_iname(knl, "k", 8, outer_tag="g.2", inner_tag="l.2" )
  
  return knl
#+end_src
 
\bibliography{cursov}
\bibliographystyle{plain}
